{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf272e33-d0dd-4612-a50d-a21699db36b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "year = 1977\n",
    "rd_actual = pd.read_excel('rdindex' + str(year) + '.xlsx')\n",
    "\n",
    "if year == 1977:\n",
    "    rd_actual = rd_actual.dropna(axis=0)\n",
    "    rd_actual['RD no.'] = pd.to_numeric(rd_actual['RD no.'].astype(str).str.split('.').str[0], errors='coerce').astype(float).astype('Int64')\n",
    "    rd_actual['rdid'] = rd_actual['RD no.'].astype(str).str[:3] + '0' + rd_actual['RD no.'].astype(str).str[3:]\n",
    "    rd_actual['rdid'] = rd_actual['rdid'].astype(int)\n",
    "else:\n",
    "    rd_actual['rdid'] = rd_actual['RD no.'].astype(str).apply(lambda x: x[:3] + '0' + x[3:]).astype(int)\n",
    "\n",
    "column_length = rd_actual['CPC code'].astype(str).str.len()\n",
    "print(\"CPC code length\", set(column_length))\n",
    "\n",
    "rd_actual.tail(2)\n",
    "# rd.rename(columns={'year': 'Year', 'Predicted_Label': 'cpc_class'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72199b3-d927-4038-9e7c-91bbf7788a36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "years = [1977, 1976, 1973, 1974, 1972]\n",
    "rd = pd.read_csv('rd_prediction_bert-base-uncased_32_batch_size_alpha_10.csv', usecols=['rdid', 'year', 'Predicted_Label'])\n",
    "rd = rd[rd['year'].isin(years)]\n",
    "rd.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c3dbc8-2796-4666-b90b-7f5e0ac656ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc9efd8-f912-4488-a36d-aea6b44d5232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# List of years\n",
    "# years = [1977, 1976, 1973, 1974, 1972]  # Add more years as needed\n",
    "\n",
    "# Create an empty list to store the DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Read and process data for each year\n",
    "for year in years:\n",
    "    filename = 'rdindex' + str(year) + '.xlsx'\n",
    "    \n",
    "    if year == 1977:\n",
    "        rd_actual = pd.read_excel(filename)\n",
    "        rd_actual = rd_actual.dropna(axis=0)\n",
    "        rd_actual['RD no.'] = pd.to_numeric(rd_actual['RD no.'].astype(str).str.split('.').str[0], errors='coerce').astype(float).astype('Int64')\n",
    "        rd_actual['rdid'] = rd_actual['RD no.'].astype(str).str[:3] + '0' + rd_actual['RD no.'].astype(str).str[3:]\n",
    "        rd_actual['rdid'] = rd_actual['rdid'].astype(int)\n",
    "        # display(rd_actual['rdid'])\n",
    "    elif year == 1976:\n",
    "        rd_actual = pd.read_excel(filename)\n",
    "        rd_actual['rdid'] = rd_actual['RD no.'].astype(str).apply(lambda x: x[:3] + '0' + x[3:]).astype(int)\n",
    "        # display(rd_actual)\n",
    "    \n",
    "    elif((year == 1973) or (year == 1974)):\n",
    "        filename = str(year) + '_RD - Sheet1.csv'\n",
    "        rd_actual = pd.read_csv(filename)\n",
    "        rd_actual['CPC code'] = rd_actual['CPC Class'].astype(str)\n",
    "        rd_actual['rdid'] = rd_actual['RD No'].astype(str).apply(lambda x: x[:3] + '0' + x[3:]).astype(int)\n",
    "        \n",
    "    elif(year == 1972):\n",
    "        filename = str(year) + '_RD - Sheet1.csv'\n",
    "        rd_actual = pd.read_csv(filename)\n",
    "        rd_actual['CPC code'] = rd_actual['CPC Class'].astype(str)\n",
    "        rd_actual['rdid'] = rd_actual['RD No'].astype(str).apply(lambda x: x[:3] + x[3:]).astype(int)\n",
    "        # display(rd_actual['rdid'])\n",
    "        # display(rd_actual)\n",
    "    dfs.append(rd_actual)\n",
    "\n",
    "# Concatenate the DataFrames into a single DataFrame\n",
    "rd_actual = pd.concat(dfs)\n",
    "\n",
    "# Print CPC code length\n",
    "column_length = rd_actual['CPC code'].astype(str).str.len()\n",
    "print(\"CPC code length:\", set(column_length))\n",
    "\n",
    "rd_actual = rd_actual[['CPC code', 'rdid']]\n",
    "rd_actual.reset_index(drop = True, inplace = True)\n",
    "# Print the tail of the merged DataFrame\n",
    "display(rd_actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b61c35a-f5e5-4972-b0d7-ff7b837b55b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #for 2 cpc code length\n",
    "# # Merge the 'cpc' column from rd_actual to rd based on 'rdid'\n",
    "# merged_df = rd.merge(rd_actual[['rdid', 'CPC code']], on='rdid', how='left')\n",
    "# display(merged_df)\n",
    "# # print(merged_df.year.value_counts())\n",
    "# merged_df = merged_df.dropna(axis=0)\n",
    "# merged_df = merged_df.reset_index(drop=True)\n",
    "# merged_df['cpc_actual'] = merged_df['CPC code'].str[:-2]\n",
    "# merged_df['Predicted_Label'] = merged_df['Predicted_Label'].str[:-1]\n",
    "# merged_df_non_null = merged_df.dropna(subset=['cpc_actual'])\n",
    "# # Remove the brackets ( ) around the text in 'cpc_actual' column\n",
    "# merged_df_non_null['cpc_actual'] = merged_df_non_null['cpc_actual'].str.replace(r\"\\(|\\)\", \"\")\n",
    "# merged_df_non_null = merged_df_non_null[merged_df_non_null['cpc_actual'].str.len() == 2]\n",
    "# # merged_df_non_null\n",
    "# # pd.set_option('display.max_rows', None)\n",
    "# # merged_df_non_null[[\"rdid\", \"Predicted_Label\", \"cpc_actual\", \"year\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779385cd-7454-4352-b0fd-194f5b2aa8eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#For 3 cpc code length\n",
    "# Merge the 'cpc' column from rd_actual to rd based on 'rdid'\n",
    "merged_df = rd.merge(rd_actual[['rdid', 'CPC code']], on='rdid', how='left')\n",
    "display(merged_df)\n",
    "# print(merged_df.year.value_counts())\n",
    "merged_df = merged_df.dropna(axis=0)\n",
    "merged_df = merged_df.reset_index(drop=True)\n",
    "merged_df['cpc_actual'] = merged_df['CPC code'].str[:-1]\n",
    "merged_df_non_null = merged_df.dropna(subset=['cpc_actual'])\n",
    "# Remove the brackets ( ) around the text in 'cpc_actual' column\n",
    "merged_df_non_null['cpc_actual'] = merged_df_non_null['cpc_actual'].str.replace(r\"\\(|\\)\", \"\")\n",
    "merged_df_non_null = merged_df_non_null[merged_df_non_null['cpc_actual'].str.len() == 3]\n",
    "# merged_df_non_null\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# merged_df_non_null[[\"rdid\", \"Predicted_Label\", \"cpc_actual\", \"year\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c64ed4b-0f26-4ffd-9254-14a7a3e360d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df_non_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bb71e7-3ea2-4f39-b56e-6e5bbe7f6230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming you have the merged_df_non_null DataFrame\n",
    "\n",
    "# Extract the predicted label and actual values columns\n",
    "y_pred = merged_df_non_null['Predicted_Label']\n",
    "y_actual = merged_df_non_null['cpc_actual']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_actual, y_pred)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_actual, y_pred, average='macro')\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_actual, y_pred, average='macro')\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_actual, y_pred, average='macro')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f1442f-d694-40fa-ac3b-78e8411afdd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the merged_df_non_null DataFrame\n",
    "\n",
    "# Extract the predicted label and actual values columns\n",
    "y_pred = merged_df_non_null['Predicted_Label']\n",
    "y_actual = merged_df_non_null['cpc_actual']\n",
    "\n",
    "# Extract the first letter from the CPC classes\n",
    "y_pred_first_letter = y_pred.apply(lambda x: x[0])\n",
    "y_actual_first_letter = y_actual.apply(lambda x: x[0])\n",
    "\n",
    "# Get the unique first letters from both labels\n",
    "unique_classes = sorted(set(y_actual_first_letter) | set(y_pred_first_letter))\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(y_actual_first_letter, y_pred_first_letter, labels=unique_classes)\n",
    "\n",
    "# Convert the confusion matrix to a DataFrame for better visualization\n",
    "cm_df = pd.DataFrame(cm, index=unique_classes, columns=unique_classes)\n",
    "\n",
    "# Sort the DataFrame by overall performance (you can choose any performance metric, e.g., accuracy, precision, recall, etc.)\n",
    "# Here, we'll sort by accuracy (sum of true positives on the diagonal) in descending order\n",
    "sum_by_class = cm_df.sum(axis=1)\n",
    "sorted_classes = sum_by_class.sort_values(ascending=False).index\n",
    "cm_df_sorted = cm_df.loc[sorted_classes, sorted_classes]\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_df_sorted, annot=True, cmap='Blues', fmt='d')\n",
    "plt.xlabel('Predicted CPC Sectors')\n",
    "plt.ylabel('Actual CPC Sectors')\n",
    "plt.title('Confusion Matrix on sectors (Ranked)')\n",
    "plt.tight_layout()\n",
    "# Save the figure in EPS format with width of 20 cm and no extra spaces\n",
    "plt.savefig('confusion_matrix_ranked_1.eps', format='eps', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52260198-6856-47cc-96a8-9134f9b71723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def class_wise_accuracy(confusion_matrix_df):\n",
    "    class_wise_accuracy_dict = {}\n",
    "    for class_label in confusion_matrix_df.index:\n",
    "        true_positives = confusion_matrix_df.loc[class_label, class_label]\n",
    "        false_negatives = confusion_matrix_df.loc[class_label, :].sum() - true_positives\n",
    "        class_accuracy = true_positives / (true_positives + false_negatives)\n",
    "        class_wise_accuracy_dict[class_label] = class_accuracy\n",
    "\n",
    "    return class_wise_accuracy_dict\n",
    "\n",
    "# Assuming that the confusion matrix is stored in a pandas DataFrame called cm_df_sorted\n",
    "# You can sort the DataFrame to make it easier to interpret the results\n",
    "cm_df_sorted = cm_df_sorted.sort_index(axis=0).sort_index(axis=1)\n",
    "\n",
    "# Calculate class-wise accuracy using the function\n",
    "class_wise_acc = class_wise_accuracy(cm_df_sorted)\n",
    "\n",
    "# Print the class-wise accuracy\n",
    "print(\"Class-wise Accuracy:\")\n",
    "for class_label, accuracy in class_wise_acc.items():\n",
    "    print(f\"Sector {class_label}: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff8a128-78d7-415f-b348-fec06dfde90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the merged_df_non_null DataFrame\n",
    "\n",
    "# Extract the predicted label and actual values columns\n",
    "y_pred = merged_df_non_null['Predicted_Label']\n",
    "y_actual = merged_df_non_null['cpc_actual']\n",
    "\n",
    "# Extract the first letter from the CPC classes\n",
    "y_pred_first_letter = y_pred.apply(lambda x: x[0:2])\n",
    "y_actual_first_letter = y_actual.apply(lambda x: x[0:2])\n",
    "\n",
    "# Get the unique first letters from both labels\n",
    "unique_classes = sorted(set(y_actual_first_letter) | set(y_pred_first_letter))\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(y_actual_first_letter, y_pred_first_letter, labels=unique_classes)\n",
    "\n",
    "# Convert the confusion matrix to a DataFrame for better visualization\n",
    "cm_df = pd.DataFrame(cm, index=unique_classes, columns=unique_classes)\n",
    "\n",
    "# Sort the DataFrame by overall performance (you can choose any performance metric, e.g., accuracy, precision, recall, etc.)\n",
    "# Here, we'll sort by accuracy (sum of true positives on the diagonal) in descending order\n",
    "sum_by_class = cm_df.sum(axis=1)\n",
    "sorted_classes = sum_by_class.sort_values(ascending=False).index\n",
    "cm_df_sorted = cm_df.loc[sorted_classes, sorted_classes]\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cm_df_sorted, annot=True, cmap='Blues', fmt='d')\n",
    "plt.xlabel('Predicted CPC Class (First Two Digits)')\n",
    "plt.ylabel('Actual CPC Class (First Two Digits)')\n",
    "plt.title('Confusion Matrix on sectors (Ranked)')\n",
    "plt.tight_layout()\n",
    "# Save the figure in EPS format with width of 20 cm and no extra spaces\n",
    "plt.savefig('confusion_matrix_ranked_2.eps', format='eps', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f28e08c-e5b2-4e62-ad7a-0586db4d2f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ce2fa-7569-4e3f-a63f-efa9cfbb0d7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4df2a8d-7f86-48d5-aa82-f9a466ddd7fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming you have the merged_df_non_null DataFrame\n",
    "\n",
    "# Extract the predicted label and actual values columns\n",
    "y_pred = merged_df_non_null['Predicted_Label'].str[0]\n",
    "y_actual = merged_df_non_null['cpc_actual'].str[0]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_actual, y_pred)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_actual, y_pred, average='macro')\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_actual, y_pred, average='macro')\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_actual, y_pred, average='macro')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c7d793-aee9-477c-8e6a-fb41fef3f723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed5711d-28af-4763-8c99-439ddeeaed67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc689857-beee-4e39-bfbd-623fd88c68e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
