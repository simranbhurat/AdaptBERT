{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19d40144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transformers is not preinstalled in google colab\n",
    "# !pip install transformers\n",
    "\n",
    "# weights and biases is not preinstalled in google colab\n",
    "# !pip install wandb -q\n",
    "\n",
    "# jupyter setup\n",
    "# % commands are \"jupyter notbook commands\" - %matplotlib inline allows for inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    " \n",
    "# import modules\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers \n",
    "# import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, Union\n",
    " \n",
    "# import modules\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers \n",
    "# import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "046c4f51-7386-4095-a297-2070955d5910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('final_patent.csv.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc7c876a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "parameter_dict = {\n",
    "    'huggingface_model': 'bert-base-uncased', \n",
    "    'epochs' : 4,\n",
    "    'batch_size': 5,\n",
    "    'dropout_finetune': 0.2,\n",
    "    'learning_rate_AdamW': 2e-5,\n",
    "    'metadata_embedding_size': 128,\n",
    "    'hidden_layer_size': 2048,\n",
    "    'seed': 101\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "584763a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = f\"{parameter_dict['huggingface_model']}-with-metadata-2hidden_layers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87b65888",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google made 2 GPU available for this notebook.\n",
      "GPU type: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "# Set up torch for colab\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda') # specify the GPU we want to use in torch\n",
    "  print('Google made {} GPU available for this notebook.'.format(torch.cuda.device_count()))\n",
    "  print('GPU type: {}'.format(torch.cuda.get_device_name()))\n",
    "else:\n",
    "  print('CPU must be used')\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85a36126-d0f1-459a-bcd7-e83db8829b70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# function to remove all digits from the abstract\n",
    "def clean_abstract(text):\n",
    "    return re.sub(r\"\\d\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed144ab-82c4-4bc7-b288-b830adef8774",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/slurm_tmpdir/job_22223128/ipykernel_128496/2001956895.py:9: FutureWarning: In a future version of pandas all arguments of StringMethods.rsplit except for the argument 'pat' will be keyword-only.\n",
      "  rd['text_all'] = rd['text_all'].str.rsplit('.', 1).str[0]\n"
     ]
    }
   ],
   "source": [
    "rd = pd.read_csv(\"abstract_title_text_RD.csv\")\n",
    "rd = rd[rd['language'] == 'en']\n",
    "rd = rd.dropna()\n",
    "rd = rd.sample(frac=100, replace=True)\n",
    "rd = rd.reset_index(drop=True)\n",
    "\n",
    "rd['text_all'] = rd['abstract'].apply(lambda x: x.lower())\n",
    "rd['text_all'] = rd['text_all'].apply(clean_abstract)\n",
    "rd['text_all'] = rd['text_all'].str.rsplit('.', 1).str[0]\n",
    "# metadata: review year ids\n",
    "rd['year'] = pd.to_datetime(rd['date']).dt.year\n",
    "year_dict = {k: k-rd['year'].min() for k in rd['year'].unique()}\n",
    "rd['year_ids'] = rd['year'].replace(year_dict)\n",
    "test_data = rd\n",
    "# tranform data to lists\n",
    "feature_text_target, feature_year_target = test_data['text_all'].to_list(), test_data['year_ids'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "579b91d2-a8e8-4f10-baa1-8d1e5f91991a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rd = rd.iloc[:4842097]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0b961d4-7715-43bd-8df8-8328947318ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample_data_df = sample_data_df.dropna(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4b52494-2b4b-4c8d-a846-c53711452752",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7d88ca5-5e2e-435a-926f-319047739560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample_data_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d820d927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/slurm_tmpdir/job_22223128/ipykernel_128496/2462011551.py:2: DtypeWarning: Columns (1,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sample_data_df = pd.read_csv('final_patent.csv')\n"
     ]
    }
   ],
   "source": [
    "# sample_data_df = pd.read_csv(path_to_input.format('sample_data.csv'))\n",
    "sample_data_df = pd.read_csv('final_patent.csv')\n",
    "sample_data_df = sample_data_df.dropna()\n",
    "\n",
    "possible_labels = sample_data_df['cpc_code'].unique()\n",
    "\n",
    "numerical_encoding_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    numerical_encoding_dict[possible_label] = index\n",
    "\n",
    "sample_data_df['cpc_class_numerical'] = sample_data_df['cpc_code'].replace(numerical_encoding_dict)\n",
    "\n",
    "sample_data_df['text'] = sample_data_df['patent_title'].astype(str) + ' ' + sample_data_df['patent_abstract'].astype(str)\n",
    "sample_data_df['text'] = sample_data_df['text'].apply(lambda x: x.lower())\n",
    "sample_data_df['text'] = sample_data_df['text'].apply(clean_abstract)\n",
    "\n",
    "# metadata: review year ids\n",
    "sample_data_df['year'] = pd.to_datetime(sample_data_df['patent_date']).dt.year\n",
    "year_dict = {k: k-sample_data_df['year'].min() for k in sample_data_df['year'].unique()}\n",
    "sample_data_df['year_ids'] = sample_data_df['year'].replace(year_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f030b40d-34aa-4bf0-9712-74b8672def93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_data_df.reset_index(inplace = True)\n",
    "sample_data_df = sample_data_df[['patent_id', 'cpc_code', 'cpc_class_numerical', 'text', 'year', 'year_ids']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32e5887d-9e8d-4bf2-b253-07fd5ca43edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample_data_df = sample_data_df_1\n",
    "sample_data_df_1 = sample_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d31ff667-bc72-4ba2-8eca-7e098a359eae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = sample_data_df.groupby(['cpc_code', 'year']).head(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49583e85-0c09-43ef-aeaa-6415880d8c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_id</th>\n",
       "      <th>cpc_code</th>\n",
       "      <th>cpc_class_numerical</th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "      <th>year_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000000</td>\n",
       "      <td>G01</td>\n",
       "      <td>0</td>\n",
       "      <td>coherent ladar using intra-pixel quadrature de...</td>\n",
       "      <td>2018</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000001</td>\n",
       "      <td>B29</td>\n",
       "      <td>1</td>\n",
       "      <td>injection molding machine and mold thickness c...</td>\n",
       "      <td>2018</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000002</td>\n",
       "      <td>B29</td>\n",
       "      <td>1</td>\n",
       "      <td>method for manufacturing polymer film and co-e...</td>\n",
       "      <td>2018</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000003</td>\n",
       "      <td>B29</td>\n",
       "      <td>1</td>\n",
       "      <td>method for producing a container from a thermo...</td>\n",
       "      <td>2018</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000004</td>\n",
       "      <td>B29</td>\n",
       "      <td>1</td>\n",
       "      <td>process of obtaining a double-oriented film, c...</td>\n",
       "      <td>2018</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842092</th>\n",
       "      <td>9999995</td>\n",
       "      <td>B29</td>\n",
       "      <td>1</td>\n",
       "      <td>method for producing molded article of fiber-r...</td>\n",
       "      <td>2018</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842093</th>\n",
       "      <td>9999996</td>\n",
       "      <td>B29</td>\n",
       "      <td>1</td>\n",
       "      <td>polymer or polymer composite membrane having t...</td>\n",
       "      <td>2018</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842094</th>\n",
       "      <td>9999997</td>\n",
       "      <td>B29</td>\n",
       "      <td>1</td>\n",
       "      <td>metal-plastic composite and method for produci...</td>\n",
       "      <td>2018</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842095</th>\n",
       "      <td>9999998</td>\n",
       "      <td>B29</td>\n",
       "      <td>1</td>\n",
       "      <td>glass-resin laminate and method for producing ...</td>\n",
       "      <td>2018</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842096</th>\n",
       "      <td>9999999</td>\n",
       "      <td>B29</td>\n",
       "      <td>1</td>\n",
       "      <td>injector for plastic material injection moldin...</td>\n",
       "      <td>2018</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4842097 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        patent_id cpc_code  cpc_class_numerical  \\\n",
       "0        10000000      G01                    0   \n",
       "1        10000001      B29                    1   \n",
       "2        10000002      B29                    1   \n",
       "3        10000003      B29                    1   \n",
       "4        10000004      B29                    1   \n",
       "...           ...      ...                  ...   \n",
       "4842092   9999995      B29                    1   \n",
       "4842093   9999996      B29                    1   \n",
       "4842094   9999997      B29                    1   \n",
       "4842095   9999998      B29                    1   \n",
       "4842096   9999999      B29                    1   \n",
       "\n",
       "                                                      text  year  year_ids  \n",
       "0        coherent ladar using intra-pixel quadrature de...  2018        42  \n",
       "1        injection molding machine and mold thickness c...  2018        42  \n",
       "2        method for manufacturing polymer film and co-e...  2018        42  \n",
       "3        method for producing a container from a thermo...  2018        42  \n",
       "4        process of obtaining a double-oriented film, c...  2018        42  \n",
       "...                                                    ...   ...       ...  \n",
       "4842092  method for producing molded article of fiber-r...  2018        42  \n",
       "4842093  polymer or polymer composite membrane having t...  2018        42  \n",
       "4842094  metal-plastic composite and method for produci...  2018        42  \n",
       "4842095  glass-resin laminate and method for producing ...  2018        42  \n",
       "4842096  injector for plastic material injection moldin...  2018        42  \n",
       "\n",
       "[4842097 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data_df = result.reset_index(drop=True)\n",
    "sample_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c07e14e-92fc-4bcf-9932-dcb49dc48812",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4842097, 6)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b06fa9c-137b-46fd-a798-31a4988ef315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4842097, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43d2db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train validation test hold out split\n",
    "# test size\n",
    "test_size = 0.2\n",
    "\n",
    "# validation size\n",
    "val_size = 0.2\n",
    "\n",
    "# training size\n",
    "train_size = 0.6\n",
    "\n",
    "training_data, test_data = sklearn.model_selection.train_test_split(sample_data_df, test_size=test_size, random_state=parameter_dict['seed'])\n",
    "training_data, validation_data = sklearn.model_selection.train_test_split(training_data, test_size=val_size/(1-test_size), random_state=parameter_dict['seed'])\n",
    "# del all_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "134724a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranform data to lists\n",
    "feature_text_train, feature_year_train, labels_train = training_data['text'].to_list(), training_data['year_ids'].to_list(), training_data['cpc_class_numerical'].to_list()\n",
    "feature_text_validation, feature_year_validation, labels_validation = validation_data['text'].to_list(), validation_data['year_ids'].to_list(), validation_data['cpc_class_numerical'].to_list()\n",
    "feature_text_test, feature_year_test, labels_test = test_data['text'].to_list(), test_data['year_ids'].to_list(), test_data['cpc_class_numerical'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "077f4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassificationMetadata(transformers.BertPreTrainedModel):\n",
    "    def __init__(self, config, num_year_ids, metadata_embedding_size, hidden_layer_size, dropout_fine_tune):\n",
    "        super().__init__(config)\n",
    "        self.bert = transformers.BertModel(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_fine_tune)\n",
    "        self.embedding_year = nn.Embedding(num_year_ids, metadata_embedding_size) #32, 128       \n",
    "        self.layer_normalizer = nn.LayerNorm(768 + metadata_embedding_size) # 896\n",
    "        self.fc1_hidden = nn.Linear(768 + metadata_embedding_size, hidden_layer_size)#896 , 2048\n",
    "        self.fc2_hidden = nn.Linear(hidden_layer_size, hidden_layer_size)#2048 , 2048\n",
    "        self.fc_classifier = nn.Linear(hidden_layer_size, config.num_labels)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        year_ids: Optional[torch.Tensor] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], transformers.modeling_outputs.SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        year_embedding = self.embedding_year(year_ids)\n",
    "\n",
    "        pooled_bert_output = outputs[1]\n",
    "        \n",
    "        # print(pooled_bert_output.size(), year_embedding.size())\n",
    "        \n",
    "        # concat\n",
    "        input_classification_head = torch.cat((pooled_bert_output, year_embedding), dim=1)#32, 1024\n",
    "\n",
    "        # fc1\n",
    "        input_fc1_hidden = self.dropout(self.layer_normalizer(input_classification_head))#32, 1024\n",
    "        output_fc1_hidden = self.fc1_hidden(input_fc1_hidden)# 32, 768\n",
    "\n",
    "        # fc2\n",
    "        input_fc2_hidden = torch.relu(self.dropout(output_fc1_hidden))\n",
    "        output_fc2_hidden = self.fc2_hidden(input_fc2_hidden) #32, 768\n",
    "\n",
    "        # logit classifier\n",
    "        input_fc_classifier = torch.relu(self.dropout(output_fc2_hidden))\n",
    "        logits = self.fc_classifier(input_fc_classifier)\n",
    "\n",
    "        # loss\n",
    "        # loss_fct = nn.CrossEntropyLoss()\n",
    "        # loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        # loss\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        else:\n",
    "            loss = None\n",
    "            \n",
    "        return transformers.modeling_outputs.SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db8c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the pre-treained BERT Tokenizer\n",
    "bert_tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Apply the tokenizer to the datasets\n",
    "feature_text_train = bert_tokenizer(feature_text_train, padding='max_length', max_length=200, truncation =True, return_tensors='pt')\n",
    "feature_text_validation = bert_tokenizer(feature_text_validation, padding='max_length', max_length=200, truncation =True, return_tensors='pt')\n",
    "feature_text_test = bert_tokenizer(feature_text_test, padding='max_length', max_length=200, truncation =True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc6c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer examples\n",
    "# print('Example of the BERT tokenizer: \\n \\t {}'.format(bert_tokenizer.decode(feature_text_train.data['input_ids'][2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae74cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader for transformers\n",
    "class AttributeDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, tokenized_text: torch.Tensor, year_ids, labels):\n",
    "    self.tokenized_features = tokenized_text\n",
    "    self.year_ids = torch.tensor(year_ids)\n",
    "    self.labels = torch.tensor(labels)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: val[idx] for key, val in self.tokenized_features.items()}\n",
    "    item['year_ids'] = self.year_ids[idx]\n",
    "    item['labels'] = self.labels[idx]\n",
    "    return item\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "train_torch = AttributeDataset(feature_text_train, feature_year_train, labels_train)\n",
    "validation_torch = AttributeDataset(feature_text_validation, feature_year_validation, labels_validation)\n",
    "test_torch = AttributeDataset(feature_text_test, feature_year_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f72687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained BERT model and push it to the GPU memory\n",
    "\n",
    "#bert_classification_model = transformers.BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(numerical_encoding_dict))\n",
    "bert_classification_model = BertClassificationMetadata.from_pretrained('bert-base-uncased', num_labels=len(numerical_encoding_dict), num_year_ids = max(year_dict.keys()), metadata_embedding_size=parameter_dict['metadata_embedding_size'], hidden_layer_size=parameter_dict['hidden_layer_size'], dropout_fine_tune=parameter_dict['dropout_finetune'])\n",
    "\n",
    "# model to GPU\n",
    "bert_classification_model.to(device)\n",
    "\n",
    "# Define the data loader for batching, batch size 16 seems to work well\n",
    "# (higher would cause memory overflow problems on the GPU)\n",
    "train_data_loader = torch.utils.data.DataLoader(train_torch, batch_size=parameter_dict['batch_size'], shuffle=True)\n",
    "validation_data_loader = torch.utils.data.DataLoader(validation_torch, batch_size=parameter_dict['batch_size'], shuffle=True)\n",
    "number_of_batches = len(train_data_loader)\n",
    "print(f'Number of Batches: {number_of_batches}')\n",
    "\n",
    "# Setup the ADAM optimizer with generic parameters.\n",
    "optimizer = torch.optim.AdamW(bert_classification_model.parameters(), lr=parameter_dict['learning_rate_AdamW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91de648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the pre-treained BERT Tokenizer\n",
    "bert_tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Apply the tokenizer to the datasets\n",
    "feature_text_target= bert_tokenizer(feature_text_target, padding='max_length', max_length=200, truncation =True, return_tensors='pt')\n",
    "\n",
    "# data loader for transformers\n",
    "class AttributeDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, tokenized_text: torch.Tensor, year_ids):\n",
    "    self.tokenized_features = tokenized_text\n",
    "    self.year_ids = torch.tensor(year_ids)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: val[idx] for key, val in self.tokenized_features.items()}\n",
    "    item['year_ids'] = self.year_ids[idx]\n",
    "    return item\n",
    "\n",
    "  def __len__(self):\n",
    "    return rd.shape[0]\n",
    "\n",
    "target_torch = AttributeDataset(feature_text_target, feature_year_target)\n",
    "target_dataloader = torch.utils.data.DataLoader(target_torch, batch_size=parameter_dict['batch_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fa0d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# milestones\n",
    "def training_milestone(number_of_milestones, number_of_batches):\n",
    "  batches_per_milestone = (number_of_batches-1)/number_of_milestones\n",
    "  milestone_list = [int(x*batches_per_milestone) for x in range(1, number_of_milestones)]\n",
    "  milestone_list.append(number_of_batches-1)\n",
    "  return milestone_list\n",
    "\n",
    "training_milestones = training_milestone(16, number_of_batches)\n",
    "\n",
    "# validation\n",
    "validation_milestones = training_milestone(4, number_of_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab64822-8c32-46ac-9f73-afe37be6ad3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mmd_loss(hidden_states_source, hidden_states_target, sigma):\n",
    "    # Compute the mean embeddings of the source and target domains\n",
    "    mean_source = torch.mean(hidden_states_source, dim=0)\n",
    "    mean_target = torch.mean(hidden_states_target, dim=0)\n",
    "\n",
    "    # Compute the MMD between the source and target domains\n",
    "    mmd = 0\n",
    "    for i in range(hidden_states_source.shape[0]):\n",
    "        for j in range(hidden_states_target.shape[0]):\n",
    "            dist = torch.sum(torch.square(hidden_states_source[i] - hidden_states_target[j]))\n",
    "            mmd += torch.exp(-dist / (2 * sigma ** 2))\n",
    "\n",
    "    return mmd / (hidden_states_source.shape[0] * hidden_states_target.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff396bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timer\n",
    "start_time = time.time()\n",
    "# initialize training\n",
    "bert_classification_model.train()\n",
    "# first loss to high values to not save model\n",
    "sum_of_val_loss_min = 9999\n",
    "# iterator for the number of validation runs\n",
    "val_run_iter = 0\n",
    "\n",
    "print(''.join([25*'-', ' Start Training ', 25*'-']))\n",
    "print(''.join([25*'-', '----------------', 25*'-']))\n",
    "alpha = 0.01\n",
    "sigma = 1.0\n",
    "for epoch in range(parameter_dict['epochs']):\n",
    "    for batch_iter, (batch_source, batch_target) in enumerate(zip(train_data_loader, target_dataloader)):\n",
    "        # batch to GPU\n",
    "        # text\n",
    "        input_ids_source, attention_mask_source, year_ids_source, labels_source = batch_source['input_ids'].to(device), batch_source['attention_mask'].to(device), batch_source['year_ids'].to(device), batch_source['labels'].to(device)\n",
    "        input_ids_target, attention_mask_target, year_ids_target = batch_target['input_ids'].to(device), batch_target['attention_mask'].to(device), batch_target['year_ids'].to(device)\n",
    "        # after each updateing set optimzer gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs_source = bert_classification_model(input_ids=input_ids_source, attention_mask=attention_mask_source, year_ids=year_ids_source, labels = labels_source)\n",
    "        outputs_target = bert_classification_model(input_ids=input_ids_target, attention_mask=attention_mask_target, year_ids=year_ids_target)\n",
    "        hidden_states_target = outputs_target.hidden_states[0]\n",
    "        hidden_states_source = outputs_source.hidden_states[0]\n",
    "        \n",
    "        mmd_loss_value = mmd_loss(hidden_states_source, hidden_states_target, sigma)\n",
    "        # Compute the distance between the source and target distributions\n",
    "        target_dist = torch.mean(torch.exp(hidden_states_target), dim=0)\n",
    "        source_dist = torch.mean(torch.exp(hidden_states_source), dim=0)\n",
    "        \n",
    "        \"\"\"\n",
    "        the code computes the source and target distributions by taking the mean of the exponential values of the hidden states. \n",
    "        The exponential function is used to convert the hidden states into a probability distribution over the vocabulary.\n",
    "        The mean is computed along the batch dimension, resulting in a single probability distribution for each domain.\n",
    "        \"\"\"\n",
    "        distance = torch.sum(torch.abs(source_dist - target_dist))\n",
    "        \"\"\"\n",
    "        Finally, the code computes the distance between the source and target distributions using the L1 distance. \n",
    "        The L1 distance is the sum of the absolute differences between the corresponding elements of the two distributions. \n",
    "        This distance is used as a measure of how different the two domains are from each other. \n",
    "        The goal of domain adaptation is to minimize this distance, typically by adjusting the model's parameters to make it more robust to domain shift.\n",
    "        \"\"\"\n",
    "\n",
    "        # total_loss = outputs_source[0] + alpha * distance\n",
    "        \n",
    "        \"\"\"\n",
    "        The MMD loss measures the difference between the source and target domains in terms of their embeddings in a high-dimensional space defined by the Gaussian kernel.\n",
    "        The goal of the loss is to minimize this difference, which encourages the model to learn embeddings that are similar across domains.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute the total loss as a linear combination of the CE and MMD losses\n",
    "        loss = outputs_source[0] + alpha * mmd_loss_value\n",
    "        # backpropergation\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "\n",
    "        # foward_pass_output['logits'] logits is the classifcation output\n",
    "        # before the softmax is applied --> the prediction is always the highest\n",
    "        # one.\n",
    "\n",
    "        # logging\n",
    "        training_log_dict = {\n",
    "            'training_loss': torch.mean(loss)\n",
    "            }\n",
    "\n",
    "        \n",
    "    # progress reporting \n",
    "    if batch_iter in training_milestones:\n",
    "      # save memory by not calculating a gradient\n",
    "      with torch.no_grad():\n",
    "        print(f'Epoch: {epoch}\\tBatch: {batch_iter}\\tTrain-Loss: {torch.mean(loss)}')\n",
    "    \n",
    "    # Validation:\n",
    "    # For each epoch I evaluate the model on a validation set twice. The model with \n",
    "    # the lowest (batch-wise sum) cross-entropy loss on the validation set is chosen.\n",
    "\n",
    "\n",
    "    if batch_iter in validation_milestones:\n",
    "      # set to evaluation (stop dropout)\n",
    "      bert_classification_model.eval()\n",
    "      print(''.join([25*'-', '----------------', 25*'-']))\n",
    "      print(f'Validation Run {val_run_iter} (Epoch {epoch}):')\n",
    "\n",
    "      # initaile selection cretarion.\n",
    "      sum_of_val_loss = 0\n",
    "      num_of_correct_predictions = 0\n",
    "      # loop over validation loss\n",
    "      for val_batch in validation_data_loader:\n",
    "        # permit pytroch to save gradients for the validation data since no \n",
    "        # backpropergation is not needed (save memory)\n",
    "\n",
    "        with torch.no_grad():\n",
    "         \n",
    "          # batch to GPU\n",
    "          # text\n",
    "          batch_text_inputids = val_batch['input_ids'].to(device)\n",
    "          batch_text_attention_mask = val_batch['attention_mask'].to(device)\n",
    "\n",
    "          # metadata\n",
    "          batch_yearids = val_batch['year_ids'].to(device)\n",
    "\n",
    "          #labels\n",
    "          labels = val_batch['labels'].to(device)\n",
    "    \n",
    "          # forward pass\n",
    "          forward_pass_output = bert_classification_model(input_ids=batch_text_inputids, attention_mask=batch_text_attention_mask, year_ids=batch_yearids, labels=labels)\n",
    "          loss = forward_pass_output[0]\n",
    "\n",
    "          # add batch specific loss\n",
    "          sum_of_val_loss += loss\n",
    "\n",
    "          # accuracy\n",
    "          num_of_correct_predictions += torch.sum(forward_pass_output['logits'].argmax(axis=1) == labels)\n",
    "\n",
    "      # log the loss and precision of the model\n",
    "\n",
    "      mean_loss_val = sum_of_val_loss/len(validation_torch)*1024\n",
    "      accuracy_val = num_of_correct_predictions/len(validation_torch)\n",
    "      validation_log_dict = {\n",
    "          'validation_loss': sum_of_val_loss/len(validation_torch)*1024,\n",
    "          'validation_accuracy': num_of_correct_predictions/len(validation_torch)\n",
    "        }\n",
    "\n",
    "      # print validation results\n",
    "      print(f'\\t{val_run_iter}\\n\\t Val-Loss: \\t{mean_loss_val}\\n\\t Val-Accu: \\t{accuracy_val}')\n",
    "\n",
    "      # update iter\n",
    "      val_run_iter += 1\n",
    "      # save the model if loss improved\n",
    "      if (sum_of_val_loss<sum_of_val_loss_min):\n",
    "        sum_of_val_loss_min = sum_of_val_loss\n",
    "        torch.save(bert_classification_model, f'{model_name}.pt')\n",
    "        print(''.join(['!'*25, ' New best Model ', '!'*25]))\n",
    "      \n",
    "      # print validation loss\n",
    "      print(''.join([25*'-', '----------------', 25*'-']))\n",
    "      print(f'Resume Training:')\n",
    "      # set to training (activate dropout)\n",
    "      bert_classification_model.train()\n",
    "\n",
    "# end training           \n",
    "bert_classification_model.eval() \n",
    "print('The Training took: ',time.time()-start_time, 'Seconds')\n",
    "del bert_classification_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7845a-783c-4b87-9166-b9ddede7905b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(target_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c89017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the fine tuned model chosen on the validation set\n",
    "classification_model = torch.load((f'{model_name}.pt'), map_location=torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b8e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore dropout for testing\n",
    "classification_model.eval()\n",
    "\n",
    "# timer\n",
    "start_time = time.time()\n",
    "\n",
    "# load test data\n",
    "test_data_loader = torch.utils.data.DataLoader(target_torch, batch_size=parameter_dict['batch_size'])\n",
    "\n",
    "# numpy arrays to store predictions and true labels.\n",
    "predictions = np.array([])\n",
    "true_labels = np.array([])\n",
    "\n",
    "# train the model\n",
    "for test_batch in test_data_loader:\n",
    "\n",
    "    # I do not need gradient\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # batch to GPU\n",
    "        # text\n",
    "        batch_text_inputids = test_batch['input_ids'].to(device)\n",
    "        batch_text_attention_mask = test_batch['attention_mask'].to(device)\n",
    "\n",
    "        # metadata\n",
    "        batch_yearids = test_batch['year_ids'].to(device)\n",
    "\n",
    "        #labels\n",
    "        # labels = test_batch['labels'].to(device)\n",
    "\n",
    "        # forward pass\n",
    "        foward_pass_output = classification_model(input_ids=batch_text_inputids, attention_mask=batch_text_attention_mask, year_ids=batch_yearids)\n",
    "        \n",
    "        # foward_pass_output['logits'] logits is the classifcation output\n",
    "        # before the softmax is applied --> the prediction is always the highest\n",
    "        # one.\n",
    "        predictions = np.append(predictions, foward_pass_output['logits'].argmax(axis=1).cpu().numpy())\n",
    "\n",
    "        # since I shuffle the batche\n",
    "        # true_labels = np.append(true_labels, labels.cpu().numpy())\n",
    "\n",
    "print('Out-of-sample predictions took: ',time.time()-start_time, 'Seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b9a12b-5d95-480e-a458-b8e944f989b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ecdb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluations\n",
    "# import tables\n",
    "# Path((f'{model_name}')).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # tests\n",
    "# test_statistic = dict()\n",
    "# test_statistic['accuracy'] = sklearn.metrics.accuracy_score(true_labels, predictions)\n",
    "# test_statistic['recall'] = sklearn.metrics.recall_score(true_labels, predictions, average='macro')\n",
    "# test_statistic['precison'] = sklearn.metrics.precision_score(true_labels, predictions, average='macro')\n",
    "# test_statistic['f1_score'] = sklearn.metrics.f1_score(true_labels, predictions, average='macro')\n",
    "\n",
    "# # # to df and safe\n",
    "# scores_df = pd.DataFrame.from_dict(test_statistic, orient='index').reset_index().rename(columns={'index': 'measure', 0: 'score'})\n",
    "# scores_df.to_csv((f'{model_name}/measures_{model_name}_test.csv'), index=False)\n",
    "# print(f'Evaluation Measures Test Set:\\n{scores_df.to_markdown()}\\n')\n",
    "\n",
    "# # get the confusion matrix\n",
    "# confusion_matrix = sklearn.metrics.confusion_matrix(true_labels, predictions)\n",
    "# confusion_matrix_plot = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix.astype('int'), display_labels=numerical_encoding_dict.keys())\n",
    "# confusion_matrix_plot.plot(values_format='.0f', xticks_rotation='vertical')\n",
    "# print('Confusion Matrix Test Set:')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig((f'{model_name}/confusion_matirx_{model_name}_test.png'), bbox_inches='tight', dpi=1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef606e59-51d3-4d5a-8bbb-d32a5b100505",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4c7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predictions.txt', 'w') as file:\n",
    "    for item in predictions:\n",
    "        file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2440eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = list(map(int, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e98134",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_keys = []\n",
    "\n",
    "for prediction in predictions_list:\n",
    "    for key, value in numerical_encoding_dict.items():\n",
    "        if value == prediction:\n",
    "            matched_keys.append(key)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5af55-0c90-4d0a-9e34-5b6f16457880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(predictions_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9772b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd['Predicted_Label'] = matched_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b6b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd_predicted = rd[['rdid', 'text_all', 'year', 'Predicted_Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c35bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd_predicted.to_csv(\"rd_predicted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744d6b2-bee3-4de8-a94a-695d127568e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b5d572-4f73-489f-a57f-30375a37e8a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
