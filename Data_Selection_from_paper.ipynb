{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a3e416-1828-49c4-8ffb-2d2777e3c62b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the first phase, a domain classifier is trained on a pair of datasets.\n",
    "The goal is to teach the model how to distinguish between the two datasets\n",
    "by learning their unique features and characteristics.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f66e8-72bd-40eb-9587-e1b52eb415e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed4bc7d-bce7-4059-8e99-966d954d5794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# function to remove all digits from the abstract\n",
    "def clean_abstract(text):\n",
    "    return re.sub(r\"\\d\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf1f80c-dca3-4272-8eb5-987809e965a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('final_patent.csv')\n",
    "data1 = data1.dropna()\n",
    "data1 = data1.reset_index(drop=True)\n",
    "data1['text'] = data1['patent_title'].astype(str) + ' ' + data1['patent_abstract'].astype(str)\n",
    "data1['text'] = data1['text'].apply(lambda x: x.lower())\n",
    "data1['text'] = data1['text'].apply(clean_abstract)\n",
    "data1[\"label\"] = \"Patents\"\n",
    "data1[[\"text\", \"label\"]] = data1[[\"patent_abstract\", \"label\"]].astype(str)\n",
    "data1[\"year\"] = pd.to_datetime(data1['patent_date']).dt.year\n",
    "data1 = data1[[\"text\", \"cpc_code\", \"year\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789ec9c4-6d99-4bf7-bd21-6ed25569db6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76148fd-827e-4ab1-9ec8-3c85d43265f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_data = data1.groupby(['cpc_code', 'year'])\n",
    "\n",
    "data1 = grouped_data.apply(lambda x: x.sample(n=min(200, len(x)), random_state=42))\n",
    "data1 = data1.reset_index(drop=True)\n",
    "data1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63fdffe-a8c3-4159-9b9f-09bc5eb57baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('abstract_title_text_RD.csv')\n",
    "data2 = data2.dropna()\n",
    "data2 = data2.reset_index(drop=True)\n",
    "\n",
    "data2['abstract'] = data2['abstract'].apply(lambda x: x.lower())\n",
    "data2['abstract'] = data2['abstract'].apply(clean_abstract)\n",
    "data2['abstract'] = data2['abstract'].str.rsplit('.', 1).str[0]\n",
    "data2['year'] = pd.to_datetime(data2['date']).dt.year\n",
    "# test_data = rd\n",
    "\n",
    "data2 = data2.dropna()\n",
    "data2 = data2.reset_index(drop=True)\n",
    "data2[\"label\"] = \"RD\"\n",
    "data2[[\"text\", \"label\"]] = data2[[\"abstract\", \"label\"]].astype(str)\n",
    "data2 = data2[[\"text\", \"year\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b891c31-b134-4bde-b308-d4f0e2d12127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49b95f-135f-4028-853e-e51bf91127e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f9308d-1a31-4eda-9091-bbeba5ee5bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "from tqdm import tqdm\n",
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=250):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        text_chunk = texts[i:i + chunk_size]\n",
    "        encs = tokenizer.batch_encode_plus(\n",
    "            text_chunk,\n",
    "            max_length=maxlen,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )        \n",
    "        input_ids.append(encs['input_ids'])\n",
    "        attention_mask.append(encs['attention_mask'])\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.cat(input_ids, dim=0).squeeze(),\n",
    "        'attention_mask': torch.cat(attention_mask, dim=0).squeeze()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3c4b17-6c9b-4115-bd0d-426baa738c66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge the datasets into a single dataframe\n",
    "data = pd.concat([data1, data2], ignore_index=True)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "train_data_text = train_data['text'].to_list()\n",
    "val_data_text = val_data['text'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c21cc-29d6-460c-a1cc-338b03fe2fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc04cd-ecb1-4c72-bcb2-e98f216d6844",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_encodings = fast_encode(train_data_text, tokenizer)\n",
    "val_encodings = fast_encode(val_data_text, tokenizer)\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b7ca5-fc5e-4364-a876-8921d3e665e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the input data and convert to tensors\n",
    "def labels(data):\n",
    "    label_map = {label: i for i, label in enumerate(set(data['label'].to_list()))}\n",
    "    labels = [label_map[label] for label in data['label'].to_list()]\n",
    "    labels = torch.tensor(labels)\n",
    "    return labels\n",
    "train_labels = labels(train_data)\n",
    "val_labels = labels(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b4038f-fd86-4e97-ac5e-314f9097bb7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the two datasets\n",
    "# data1 = pd.read_csv('sample_data.csv', nrows = 4000)\n",
    "# data1['text'] = data1['patent_title'].astype(str) + ',' + data1['patent_abstract'].astype(str) + ',' + data1['summary_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4d6c23-2bad-4364-a226-0ed195684ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "num_epochs = 1\n",
    "total_steps = len(train_data) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Reset the loss for this epoch\n",
    "    total_loss = 0\n",
    "    total_mask = []\n",
    "    all_diffs = []\n",
    "    \n",
    "    # Train the model on batches of data\n",
    "    for i in range(0, len(train_data), 32):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move the data to the device\n",
    "        batch_encodings = {key: val[i:i+32].to(device) for key, val in train_encodings.items()}\n",
    "        batch_labels = train_labels[i:i+32].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch_encodings, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        # print(outputs.logits)\n",
    "        # apply softmax along the second dimension (classes)\n",
    "        probs = F.softmax(outputs.logits, dim=1)\n",
    "        # convert probabilities tensor to a NumPy array\n",
    "        probs_np = probs.detach().cpu().numpy()\n",
    "        diff = abs(probs_np[:, 1] - probs_np[:, 0]).tolist()\n",
    "        all_diffs.append(diff)\n",
    "\n",
    "        # # create boolean mask to select rows with probability between 0.5 and 0.7\n",
    "        # mask = np.logical_and(probs_np[:,0] > 0.4, probs_np[:,0] < 0.6)\n",
    "        # total_mask.append(mask)\n",
    "        # # select rows using boolean mask\n",
    "        # selected_rows = data1[mask]\n",
    "\n",
    "        # # print selected rows\n",
    "        # print(selected_rows)\n",
    "        # print(probs)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    print(f\"Epoch {epoch+1} loss: {total_loss/len(train_data)}\")\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        total = 0\n",
    "        total_correct = 0\n",
    "        num_correct = 0\n",
    "        for i in range(0, len(val_data), 32):\n",
    "            # Move the data to the device\n",
    "            batch_encodings = {key: val[i:i+32].to(device) for key, val in val_encodings.items()}\n",
    "            batch_labels = val_labels[i:i+32].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(**batch_encodings, labels=batch_labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            total = total + len(predictions)\n",
    "            num_correct = torch.sum(predictions == batch_labels).item()\n",
    "            total_correct = total_correct + num_correct\n",
    "        print(total_correct/total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61aadd-0130-45b3-b082-c62cc2ed0c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the second phase, the source domain training samples are ranked based on the output from the domain classifier. \n",
    "This ranking process identifies which samples in the source domain are most similar to the target domain. \n",
    "A subset of the top-ranked data points is then selected from the source domain training set.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4473fb-7b45-4d99-8077-a481681d9d03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "flattened = list(itertools.chain.from_iterable(all_diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a0367f-c0eb-4e46-8e61-11b98dd88bac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data['diff'] = flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9b36a-b431-4240-bef9-b12f77f11dc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sort the dataframe by the 'diff' column in ascending order\n",
    "train_data_sorted = train_data.sort_values('diff')\n",
    "\n",
    "# show only the rows where the 'label' column is 'Patent'\n",
    "patent_rows = train_data_sorted[train_data_sorted['label'] == 'Patents']\n",
    "patent_rows.reset_index(inplace = True)\n",
    "# print the resulting dataframe\n",
    "display(patent_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
